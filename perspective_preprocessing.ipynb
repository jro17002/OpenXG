{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Preprocessing\n",
    "We aim to have all photos preprocessed into a normalized orientation and perspective to derive theta angle for shot chances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding theta for xG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A component of any xG model is angle to goal to derive this angle we use both goal posts and position of player to derive a theta. Simple, the larger the theta the btter the chance. Out problem is how do we derive this theta since we do not have the luxury of a standard overhead view, instead photos from different camera angles, perspectives and orientations. \n",
    "\n",
    "\n",
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"./visualizations/theta_vis.png\" alt=\"2D Representation\" style=\"height: 250px;\"/> \n",
    "        <center> \n",
    "                <p> 2D Representation </p> \n",
    "        </center>\n",
    "</td>\n",
    "<td> <img src=\"./visualizations/irl_theta_vis.png\" alt=\"Real World Example\" style=\"height: 250px;\"/>\n",
    "        <center> \n",
    "                <p> Real World Example </p> \n",
    "        </center>\n",
    "</td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homography Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily we have standarized anchors that we can use to transform our photos to commonly align with each other. We will use the 6-yard box (and possibly the 16-yard box as well) to transform on the xy plane, we are unconcerned about the z-axis at the moment.\n",
    "\n",
    "First we must obtain the keypoints for our anchor and input photos. Use ```coordinate_finder.py``` to find these keypoints.\n",
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"./visualizations/anchor_pts_vis.png\" alt=\"Drawing\" style=\"height: 250px;\"/> \n",
    "    <center> \n",
    "            <p> Base Image, with Anchor Points </p> \n",
    "    </center>\n",
    "</td>\n",
    "<td> <img src=\"./visualizations/input_pts_vis.png\" alt=\"Drawing\" style=\"height: 250px;\"/> \n",
    "    <center> \n",
    "            <p> Input Image, with Input Points </p> \n",
    "    </center>\n",
    "</td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take two photos, one being our anchor and the other the input we wish to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Complete\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "input_pts = np.float32([[274, 136], [162, 151], [452, 247], [577, 226], [131, 94], [134,286]]) # take 4 anchors from 6-yard box plus 2 extra from 16-yard box\n",
    "# input_pts_goalframe = np.float32([[355,98], [355,160], [475,195], [478, 130]]) \n",
    "# input_pts_6yd = np.float32([[274, 136], [162, 151], [452, 247], [577, 226]])\n",
    "\n",
    "anchor_pts = np.float32([[421, 178], [311, 187], [602, 246], [717, 233], [278, 152], [282, 265]]) # take 4 anchors from 6-yard box plus 2 extra from 16-yard box\n",
    "# anchor_pts_goalframe = np.float32([[500,130], [500,193], [625,216], [625, 150]])  \n",
    "# anchor_pts_6yd = np.float32([[421, 178], [311, 187], [602, 246], [717, 233]]) \n",
    "\n",
    "img_input = cv2.imread('./lfc_home_goals_21/1.PNG') \n",
    "width = 720\n",
    "height = 360\n",
    "dim = (width, height)\n",
    "\n",
    "resized_in = cv2.resize(img_input, dim, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "M,_ = cv2.findHomography(input_pts, anchor_pts)\n",
    "\n",
    "out_M = cv2.warpPerspective(resized_in, M, (resized_in.shape[1],resized_in.shape[0]))\n",
    "out_M = cv2.cvtColor(out_M, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('./visualizations/transform_MN.png', cv2.cvtColor(out_M, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Homography Transformation we get the following image.\n",
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"./visualizations/anchor_pts_vis.png\" alt=\"Drawing\" style=\"height: 250px;\"/> \n",
    "    <center> \n",
    "            <p> Anchor Image </p> \n",
    "    </center>\n",
    "</td>\n",
    "<td> <img src=\"./visualizations/transform_vis.png\" alt=\"Drawing\" style=\"height: 250px;\"/> \n",
    "    <center> \n",
    "            <p> Transformed Image </p> \n",
    "    </center>\n",
    "</td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a short demonstartion clip.\n",
    "<center>\n",
    "<img src='./visualizations/transform_vis.gif' width= '720'/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1243fa9230992003ce9b8fa3e079f994d0773efd02bf5620ca41228215347321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
